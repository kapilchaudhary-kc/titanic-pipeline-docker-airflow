# ğŸš€ **Titanic Survival Prediction Pipeline with Apache Airflow**

This project implements a **Titanic Survival Prediction Pipeline** using **Apache Airflow**, **Docker**, and **Python**. The pipeline performs the following steps:

- **Data Validation**: Ensures the schema, checks for missing values, and identifies duplicates.
- **Data Preprocessing**: Applies feature engineering, handles missing values, and prepares the data for model training and testing.
- **Model Training**: Uses Logistic Regression to train the model on the Titanic training data.
- **Model Validation & Prediction**: Tests the model on both labeled and unlabeled test datasets and generates submission files.

---

## âš™ï¸ **Directory Structure**

```
â”œâ”€â”€ dags/                          # Airflow DAGs
â”‚   â”œâ”€â”€ __init__.py                # Enables the module import/export
â”‚   â”œâ”€â”€ pipeline.py                # Airflow DAG definition
â”‚   â””â”€â”€ func/                      # Data processing and ML functions
â”‚       â”œâ”€â”€ __init__.py            # Enables the module import/export
â”‚       â”œâ”€â”€ ml_pipeline.py         # Data preprocessing, training, and testing functions
â”‚       â””â”€â”€ data_validator.py      # Data validation functions
â”œâ”€â”€ data/                          # Data directory (train and test CSVs)
â”‚   â””â”€â”€ train/                     # Training Data and Trained Model Directory
â”‚       â”œâ”€â”€ train.csv              # Titanic training data
â”‚       â”œâ”€â”€ processed_train.pkl    # Preprocessed training data
â”‚       â””â”€â”€ processed_train.pkl    # Trained Model
â”‚   â””â”€â”€ test/                      # Testing/Validation Data Directory
â”‚       â”œâ”€â”€ test.csv               # Titanic test data
â”‚       â””â”€â”€ processed_test.pkl     # Preprocessed test data
â”‚   â””â”€â”€ submission/                # Model predictions saved as CSV
â”œâ”€â”€ logs/                          # Airflow logs
â”œâ”€â”€ plugins/                       # Airflow plugins
â”œâ”€â”€ Dockerfile                     # Docker configuration
â”œâ”€â”€ docker-compose.yaml            # Airflow services configuration
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # Project documentation
```

---

## ğŸš€ **Setup and Execution**

### âœ… **Prerequisites**

- Docker and Docker Compose installed
- Python 3.x installed

---

### ğŸ”¥ **1ï¸âƒ£ Build and Run the Project**

1. **Build the Docker containers**

```bash
docker-compose build --no-cache
```

2. **Start the Airflow services**

```bash
docker-compose up -d
```

3. **Access the Airflow UI**

- Open your browser and go to:
  ```
  http://localhost:8080
  ```
- Username: `airflow`
- Password: `airflow`

---

### ğŸ”¥ **2ï¸âƒ£ File Descriptions**

#### **1. **`` - Data Validation Module

- Validates the schema, checks for missing values, and detects duplicates.
- Generates a validation report with:
  - **Schema Validation**
  - **Duplicate Rows**
  - **Missing Values**

âœ… **Usage Example:**

```python
from func import data_validator
validator = data_validator.Validator(df)
report = validator.validate()
```

---

#### **2. **`` - ML Pipeline

- ``: Prepares the Titanic dataset by applying:
  - Schema validation
  - Duplicate removal
  - Feature engineering (e.g., `FamilySize`, `IsAlone`)
  - One-hot encoding
  - Missing value imputation
- ``:
  - Trains a Logistic Regression model on the training data.
  - Splits the data into training and validation sets.
  - Saves the trained model as `titanic_model.pkl`.
- ``:
  - Tests the model on labeled or unlabeled test data.
  - For labeled data:
    - Returns accuracy and predictions.
  - For unlabeled data:
    - Generates a Kaggle-style submission file.

âœ… **Usage Example:**

```python
from func.ml_pipeline import prepare_data, train_model, test_model

# Prepare data
train_df = prepare_data('/data', 'train')

# Train model
model, accuracy = train_model('/data')

# Test model
predictions, accuracy = test_model('/data', 'titanic_model.pkl')
```

---

#### **3. **`` - Airflow DAG

- Defines the Airflow DAG with three tasks:
  - `prepare_data_task`: Prepares the Titanic data.
  - `model_training_task`: Trains the model and stores it in XCom.
  - `model_validation_task`: Validates the model and creates submission files.
- Automatically handles labeled and unlabeled test sets.

âœ… **DAG Execution:**

- DAG ID: `titanic_survival_classifier`
- Schedule: `@monthly`
- Start Date: `2025-03-25`
- Dependencies:
  ```
  prepare_data_task >> model_training_task >> model_validation_task
  ```

---

#### **4. **`` - Docker Compose Configuration

- Defines the Airflow cluster using:
  - **PostgreSQL**: Backend database.
  - **Redis**: Message broker for Celery.
  - **Airflow Webserver, Scheduler, Worker**: Manages DAG execution.
  - **Flower**: Celery monitoring UI.
- Mounts the volumes:
  ```
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./dags/func:/opt/airflow/dags/func
    - ./data:/opt/airflow/data
  ```

âœ… **Docker Services:**

- **Airflow Webserver:** `localhost:8080`
- **Flower UI:** `localhost:5555`

---

#### **5. **`` - Python Dependencies

- Contains the required Python libraries:

```
pandas  
numpy  
scikit-learn
```

---

#### **6. **``

- Specifies the base Airflow image:

```dockerfile
FROM apache/airflow:2.1.1
COPY requirements.txt .
RUN pip install -r requirements.txt
```

---

## ğŸš€ **Usage Examples**

### ğŸ› ï¸ **Train the Model**

1. Run the pipeline.
2. The model will be saved as:

```
/data/train/titanic_model.pkl
```

### ğŸ› ï¸ **Validate the Model**

- For labeled test data:

```
Test Accuracy: 0.84
```

- For unlabeled test data:

```
Submission saved at /data/submission/submission.csv
```

---

## âœ… **Airflow Commands**

- **Stop Airflow:**

```bash
docker-compose down
```

- **Clear Docker Cache:**

```bash
docker system prune -a
```

- **Check Logs:**

```bash
docker-compose logs -f
```

---

## ğŸš€ **Key Features**

- **Modularized ML Pipeline:**
  - Validation, preprocessing, training, and testing as separate functions.
- **Automated DAG Execution:**
  - Prepares data, trains, and validates the model automatically.
- **Flexible Handling:**
  - Supports both labeled and unlabeled test data.
- **Dockerized Deployment:**
  - Easily deployable using Docker Compose.

---

## ğŸ“Š **Results**

- Generates predictions for both labeled and unlabeled datasets.
- Creates a Kaggle-style submission file for unlabeled data.
- Displays model accuracy for labeled test data.

---

- âœ¨ **[KAPIL CHAUDHARY]** â€“ Developer and Machine Learning Engineer
- ğŸ“§ Contact: [kapilchaudhary1707@gmail.com](mailto\:kapilchaudhary1707@gmail.com)

---

## ğŸ“š **References**

- [Apache Airflow](https://airflow.apache.org/)
- [Running Airflow in Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
- [Titanic Dataset - Kaggle](https://www.kaggle.com/c/titanic/data)

---
